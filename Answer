library(readr)
#library(tidyverse)
library(caret)
library(fastDummies)
library(ggplot2)
library(factoextra)
library(cowplot)
library(e1071)
library(knitr)
library(ggcorrplot)
library(corrplot)
library(dplyr)
library(tidyr)

# Reading the file
Data_Science_Evaluation <- read_csv("C:/Users/KUNAL/Desktop/DS_INTER/Data Science Evaluation.csv")
summary(Data_Science_Evaluation)
#Checking for any NA values
any(colSums(is.na(Data_Science_Evaluation)) != 0)

Data_Science_Evaluation <- na.omit(Data_Science_Evaluation)

str(Data_Science_Evaluation)
#Checking for outliers
Data_Science_Evaluation %>% select_if(is.numeric) %>% mutate_all(scale) %>% gather("features","values") %>% na.omit() %>% 
  ggplot(aes(x = features, y = values)) +
  geom_boxplot(show.legend = FALSE) +
  stat_summary(fun = mean, geom = "point", pch = 1) + # Add average to the boxplot
  scale_y_continuous(name = "Variable values", minor_breaks = NULL) +
  scale_fill_brewer(palette = "Set1") +
  coord_flip() + 
  theme_minimal() +
  labs(x = "Variable names") +
  ggtitle(label = "Distribution of numeric variables in Churn dataset")


#We don't see many outliers so we can proceed with the data and scale it
#Just checking the correlation between the values
corrplot(cor(Data_Science_Evaluation %>% select_if(is.numeric)), type="upper", order="hclust",method="circle")

#Creating factor,dummies and scaling the data
Data_Science_Evaluation_factors <- Data_Science_Evaluation %>%
  select(Region,`Item Type`, `Sales Channel`, `Order Priority`) %>%
  mutate_all(.funs = function(x){as.factor((x))})

Data_Science_Evaluation_factors_dummy <- dummy_cols(Data_Science_Evaluation_factors) %>% select(-c(Region,`Item Type`, `Sales Channel`, `Order Priority`))

Data_Science_Evaluation_new <- Data_Science_Evaluation %>%
  select(-c(Country,Region,`Order Date`,`Ship Date`,`Item Type`, `Sales Channel`, `Order Priority`)) %>% 
  cbind(Data_Science_Evaluation_factors_dummy)
set.seed(123)
str(Data_Science_Evaluation_new)

Data_Science_Evaluation_new_scale <- scale(Data_Science_Evaluation_new)

#Checking the optimal number of cluster
'''
data_sci_kmeans <- kmeans(Data_Science_Evaluation_new_scale, centers = 5, nstart=25)
fviz_cluster(data_sci_kmeans, data= Data_Science_Evaluation_new_scale)
data_sci_kmeans <- kmeans(Data_Science_Evaluation_new_scale, centers = 3, nstart=25)
fviz_cluster(data_sci_kmeans, data= Data_Science_Evaluation_new_scale)
'''

data_sci_kmeans <- kmeans(Data_Science_Evaluation_new_scale, centers = 4, nstart=25)
fviz_cluster(data_sci_kmeans, data= Data_Science_Evaluation_new_scale)
set.seed(123)
#Just checking the clusters and we can see the cluster 4 is the outliers

#Adding the clusters value to the  data
Data_Science_Evaluation_copy<-Data_Science_Evaluation
Data_Science_Evaluation_copy<- cbind(Data_Science_Evaluation_copy,Cluster=data_sci_kmeans$cluster)

Data_Science_Evaluation_new_scale_cluster4<-Data_Science_Evaluation_copy %>% 
  filter(Data_Science_Evaluation_copy["Cluster"]==4)
Data_Science_Evaluation_new_scale_cluster3<-Data_Science_Evaluation_copy %>% 
  filter(Data_Science_Evaluation_copy["Cluster"]==3)
Data_Science_Evaluation_new_scale_cluster2<-Data_Science_Evaluation_copy %>% 
  filter(Data_Science_Evaluation_copy["Cluster"]==2)
Data_Science_Evaluation_new_scale_cluster1<-Data_Science_Evaluation_copy %>% 
  filter(Data_Science_Evaluation_copy["Cluster"]==1)

summary(Data_Science_Evaluation_new_scale_cluster4)
summary(Data_Science_Evaluation_new_scale_cluster3)
summary(Data_Science_Evaluation_new_scale_cluster2)
summary(Data_Science_Evaluation_new_scale_cluster1)
#After analyzing all the cluster's we can see that cluster 1 has minimum % to profit ratio. Hence, we could eliminate 
#few items from every location that are captured in cluster 1

#let's analyze further cluster 1 to make a specific decision
Data_Science_Evaluation_new_scale_cluster1_group<-Data_Science_Evaluation_new_scale_cluster1 %>% 
  group_by(Data_Science_Evaluation_new_scale_cluster1$`Item Type`) %>% 
  summarise(Total_Profit_Cluster1_Iteams= sum(`Total Profit`))
Data_Science_Evaluation_new_scale_cluster1_group
#we can see the minimum profit is generated by the fruits in each region
#Hence we can reduce the selling of fruits from several region that have minimum sales profit from fruits
Data_Science_Evaluation_new_scale_cluster1_fruits<-Data_Science_Evaluation_new_scale_cluster1 %>% 
  filter(`Total Profit`<mean(`Total Profit`))

Data_Science_Evaluation_new_scale_cluster1_fruits_COuntries<-Data_Science_Evaluation_new_scale_cluster1_fruits %>% 
  group_by(Region) %>% summarise(T.Profit=sum(`Total Profit`)) %>% arrange(desc(T.Profit))

  
#1 Fruits should be reduced from NorTh america.
#Personal care should be sold more because it helps to generate high revenue.
#Central America and the Caribbean, Australia and Oceania, North America should be focused more and should be given 
#more given more preference so that Total profit can be increased from these places



#Question2
Data_Science_Evaluation_copy["Total Days of shipment"]<- as.Date(Data_Science_Evaluation_copy$`Ship Date`,format="%m/%d/%Y")-as.Date(Data_Science_Evaluation_copy$`Order Date`,format="%m/%d/%Y")

Data_Science_Evaluation_copy %>% 
  group_by(`Item Type`,Region) %>% 
  summarise(td<-sum(`Total Days of shipment`)) %>% 
  filter(`td <- sum(\`Total Days of shipment\`)`>mean(`td <- sum(\`Total Days of shipment\`)`))
  
# Baby products, beverage and cereals are easiest to sell
#No! each region has totally different relation with the given product
